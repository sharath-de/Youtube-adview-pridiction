{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejumYLvuDNkf"
   },
   "source": [
    "# Bagging Algorithms**\n",
    "Bootstrap Aggregation or bagging involves taking multiple samples from your training dataset and training a model for each sample. \n",
    "The final output prediction is averaged across the predictions of all of the sub-models.\n",
    "\n",
    "The three bagging models covered in this section are as follows:\n",
    "\n",
    "\n",
    "*   Bagged Decision Trees\n",
    "\n",
    "*   Random Forest\n",
    "*   Extra TreesList item\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DalYrlzMBIWJ"
   },
   "source": [
    "#Bagged Decision Trees\n",
    "Bagging performs best with algorithms that have high variance. A popular example are decision trees, often constructed without pruning.\n",
    "\n",
    "In the example below see an example of using the BaggingClassifier with the Classification and Regression Trees algorithm (DecisionTreeClassifier). A total of 100 trees are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUCcMvdptgTU"
   },
   "outputs": [],
   "source": [
    "# Bagged Decision Trees for Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "ZW9rF5XZB_Mr",
    "outputId": "a3cfba7b-095f-41d8-c08b-e7fbc82e7e6f"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iFv444KPB_8D"
   },
   "outputs": [],
   "source": [
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "cart = DecisionTreeClassifier()\n",
    "num_trees = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "j7nSls7jCD41",
    "outputId": "c40e8267-baf9-4b5f-b65e-720d298e87bb"
   },
   "outputs": [],
   "source": [
    "model = BaggingClassifier(base_estimator=cart, n_estimators=num_trees)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYaO_96sBO1w"
   },
   "source": [
    "# Random Forest\n",
    "\n",
    "Random forest is an extension of bagged decision trees.\n",
    "\n",
    "Samples of the training dataset are taken with replacement, but the trees are constructed in a way that reduces the correlation between individual classifiers. Specifically, rather than greedily choosing the best split point in the construction of the tree, only a random subset of features are considered for each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JcuSCbbg_NAx"
   },
   "outputs": [],
   "source": [
    "# Random Forest Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T9k0ViFSCJdV"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eRUbHNKgCLpI"
   },
   "outputs": [],
   "source": [
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 50\n",
    "max_features = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "2HCx4nvDCNZD",
    "outputId": "be03b6a5-3081-47a8-b997-41d3049b275b"
   },
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=15, random_state=seed)\n",
    "model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5SfkQEoaBWWI"
   },
   "source": [
    "#Extra Trees\n",
    "Extra Trees are another modification of bagging where random trees are constructed from samples of the training dataset.\n",
    "\n",
    "You can construct an Extra Trees model for classification using the ExtraTreesClassifier class.\n",
    "\n",
    "The example below provides a demonstration of extra trees with the number of trees set to 100 and splits chosen from 7 random features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3PwMoBwGAbgF"
   },
   "outputs": [],
   "source": [
    "# Extra Trees Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubXaV1c1CQdd"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Kvf8g6iCTRm"
   },
   "outputs": [],
   "source": [
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 100\n",
    "max_features = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "JzpRk7pNCVN8",
    "outputId": "921f123f-3782-4023-bfb5-1da996326744"
   },
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = ExtraTreesClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iXKifiFVD11z"
   },
   "source": [
    "# Boosting Algorithms\n",
    "Boosting ensemble algorithms creates a sequence of models that attempt to correct the mistakes of the models before them in the sequence.\n",
    "\n",
    "Once created, the models make predictions which may be weighted by their demonstrated accuracy and the results are combined to create a final output prediction.\n",
    "\n",
    "The two most common boosting ensemble machine learning algorithms are:\n",
    "\n",
    "\n",
    "\n",
    "*   AdaBoost\n",
    "*   Stochastic Gradient Boosting\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HmqLogMWBcht"
   },
   "source": [
    "# AdaBoost\n",
    "AdaBoost was perhaps the first successful boosting ensemble algorithm. It generally works by weighting instances in the dataset by how easy or difficult they are to classify, allowing the algorithm to pay or or less attention to them in the construction of subsequent models.\n",
    "\n",
    "AdaBoost stands for ‘Adaptive Boosting’ which transforms weak learners or predictors to strong predictors in order to solve problems of classifications.\n",
    "\n",
    "You can construct an AdaBoost model for classification using the AdaBoostClassifier class.\n",
    "\n",
    "The example below demonstrates the construction of 30 decision trees in sequence using the AdaBoost algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2uYbjj8AkZl"
   },
   "outputs": [],
   "source": [
    "# AdaBoost Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXcZPtQgCZvs"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5VSfrR50CcMb"
   },
   "outputs": [],
   "source": [
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NCXFae23Cdz6"
   },
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = AdaBoostClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-iiEJBABk7V"
   },
   "source": [
    "# Gradient Boosting\n",
    "Gradient Boosting (also called Gradient Boosting Machines) are one of the most sophisticated ensemble techniques. It is also a technique that is proving to be perhaps of the the best techniques available for improving performance via ensembles.\n",
    "\n",
    "You can construct a Gradient Boosting model for classification using the GradientBoostingClassifier class.\n",
    "\n",
    "The example below demonstrates Stochastic Gradient Boosting for classification with 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OStH7h_QAyQQ"
   },
   "outputs": [],
   "source": [
    "# Stochastic Gradient Boosting Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZabxaB_wCgCP"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T8N_5AvyCict"
   },
   "outputs": [],
   "source": [
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "num_trees = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ZgY2aYtCj-7"
   },
   "outputs": [],
   "source": [
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "model = GradientBoostingClassifier(n_estimators=num_trees, random_state=seed)\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xDuT1XiABpiO"
   },
   "source": [
    "# Voting Ensemble \n",
    "Voting is one of the simplest ways of combining the predictions from multiple machine learning algorithms.\n",
    "The code below provides an example of combining the predictions of logistic regression, classification and regression trees and support vector machines together for a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "guAyDcoBA85p"
   },
   "outputs": [],
   "source": [
    "# Voting Ensemble for Classification\n",
    "import pandas\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nX9JPivCoXj"
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cxmXqebCrAS"
   },
   "outputs": [],
   "source": [
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "seed = 7\n",
    "kfold = model_selection.KFold(n_splits=10, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VyX-twN7CtIo"
   },
   "outputs": [],
   "source": [
    "\n",
    "# create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ib_Wwba9CvUk"
   },
   "outputs": [],
   "source": [
    "# create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "results = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqOcZnIx8s09"
   },
   "source": [
    "# Overfitting & Underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dLrXdw1b9yw7"
   },
   "source": [
    "For training data we are going to use the Titanic data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAuuxP1_8sXO"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, learning_curve, validation_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h86jFwCg96_h"
   },
   "source": [
    "The next step sets up plots to look the way I like them, you can ignore it or use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWFA25W280cD"
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "# Default parameters for matplotlib plots\n",
    "mpl.rcParams['xtick.labelsize'] = 22\n",
    "mpl.rcParams['ytick.labelsize'] = 22\n",
    "mpl.rcParams['figure.figsize'] = (10, 8)\n",
    "mpl.rcParams['axes.facecolor'] = (0.9,0.9,0.9)\n",
    "mpl.rcParams['lines.linewidth'] = 2\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "mpl.rcParams['grid.color'] = 'w'\n",
    "mpl.rcParams['xtick.top'] = True\n",
    "mpl.rcParams['ytick.right'] = True\n",
    "mpl.rcParams['grid.linestyle'] = '--'\n",
    "mpl.rcParams['legend.fontsize'] = 22\n",
    "mpl.rcParams['legend.facecolor'] = [1,1,1]\n",
    "mpl.rcParams['legend.framealpha'] = 0.75\n",
    "mpl.rcParams['axes.labelsize'] = 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MhgKXOq89-XE"
   },
   "source": [
    "Now we read in the data, and combine training and test sets into one. We also set up or training dataframe X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ok2qx-084QM"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_comb = df_train.append(df_test)\n",
    "X = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YVXnkAVG-DM5"
   },
   "source": [
    "Now we set up some functions to encode the data in the set into a machine useable format. In this case we turn gender into a numerical function and set family sizes so that they can be 1, 2, 3 or 4. All families larger than 3 are treated as 4; this reduces the spread in the family size variable and increases its significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTxAvNdS9HgJ"
   },
   "outputs": [],
   "source": [
    "def encode_sex(x):\n",
    "    return 1 if x == 'female' else 0\n",
    "\n",
    "def family_size(x):\n",
    "    size = x.SibSp + x.Parch \n",
    "    return 4 if size > 3 else size\n",
    "\n",
    "X['Sex'] = df_comb.Sex.map(encode_sex)\n",
    "X['Pclass'] = df_comb.Pclass\n",
    "X['FamilySize'] = df_comb.apply(family_size, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTpzxcoY-H2b"
   },
   "source": [
    "We set up new dataframes containing fares and ages and add them to the dataframe X, these will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lx5_lRKt9Ivf"
   },
   "outputs": [],
   "source": [
    "fare_median = df_train.groupby(['Sex', 'Pclass']).Fare.median()\n",
    "fare_median.name = 'FareMedian'\n",
    "\n",
    "age_mean = df_train.groupby(['Sex', 'Pclass']).Age.mean()\n",
    "age_mean.name = 'AgeMean'\n",
    "\n",
    "def join(df, stat):\n",
    "    return pd.merge(df, stat.to_frame(), left_on=['Sex', 'Pclass'], right_index=True, how='left')\n",
    "\n",
    "X['Fare'] = df_comb.Fare.fillna(join(df_comb, fare_median).FareMedian)\n",
    "X['Age'] = df_comb.Age.fillna(join(df_comb, age_mean).AgeMean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJuYDl7f-K_k"
   },
   "source": [
    "Next we discritize the data, that is we separate up groups of passengers by the fare that they paid and their age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jfGYryxl9MIl"
   },
   "outputs": [],
   "source": [
    "def quantiles(series, num):\n",
    "    return pd.qcut(series, num, retbins=True)[1]\n",
    "\n",
    "def discretize(series, bins):\n",
    "    return pd.cut(series, bins, labels=range(len(bins)-1), include_lowest=True)\n",
    "    \n",
    "X['Fare'] = discretize(X.Fare, quantiles(df_comb.Fare, 10))\n",
    "X['Age'] = discretize(X.Age, quantiles(df_comb.Age, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vmq6HW74-OOA"
   },
   "source": [
    "We split the data again into training and test sets, on the same boundary as at the start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G2zSQNvD9Q4P"
   },
   "outputs": [],
   "source": [
    "X_train = X.iloc[:df_train.shape[0]]\n",
    "X_test = X.iloc[df_train.shape[0]:]\n",
    "\n",
    "y_train = df_train.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kW2lkqmo-Rto"
   },
   "source": [
    "Set up the model\n",
    "We now set up our Random Forest model. We also specify that we want to perform 7-fold cross validation for fitting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XcGOyvyU9ZnB"
   },
   "outputs": [],
   "source": [
    "clf_1 = RandomForestClassifier(n_estimators=100, bootstrap=True, random_state=0)\n",
    "clf_1.fit(X_train, y_train)\n",
    "# Number of folds for cross validation\n",
    "num_folds = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ymqg5Cp-WsF"
   },
   "source": [
    "Some functions to plot the data\n",
    "Utility to plot training and validation or test scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XCBZJS2j9cH5"
   },
   "outputs": [],
   "source": [
    "def plot_curve(ticks, train_scores, test_scores):\n",
    "    train_scores_mean = -1 * np.mean(train_scores, axis=1)\n",
    "    train_scores_std = -1 * np.std(train_scores, axis=1)\n",
    "    test_scores_mean = -1 * np.mean(test_scores, axis=1)\n",
    "    test_scores_std = -1 * np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.fill_between(ticks, \n",
    "                     train_scores_mean - train_scores_std, \n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"b\")\n",
    "    plt.fill_between(ticks, \n",
    "                     test_scores_mean - test_scores_std, \n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.plot(ticks, train_scores_mean, 'b-', label='Training score')\n",
    "    plt.plot(ticks, test_scores_mean, 'r-', label='Test score')\n",
    "    plt.legend(fancybox=True, facecolor='w')\n",
    "\n",
    "    return plt.gca()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wfmJuUjA-bnn"
   },
   "source": [
    "And a utility to plot the validation curve of a classifier for training set X and target y. The parameter and its value range can be specified with param_name and param_range, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FwdCsVDK9eax"
   },
   "outputs": [],
   "source": [
    "def plot_validation_curve(clf, X, y, param_name, param_range, scoring='roc_auc'):\n",
    "    plt.xkcd()\n",
    "    ax = plot_curve(param_range, *validation_curve(clf, X, y, cv=num_folds, \n",
    "                                                   scoring=scoring, \n",
    "                                                   param_name=param_name, \n",
    "                                                   param_range=param_range, n_jobs=-1))\n",
    "    ax.set_title('')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xlim(2,12)\n",
    "    ax.set_ylim(-0.97, -0.83)\n",
    "    ax.set_ylabel('Error')\n",
    "    ax.set_xlabel('Model complexity')\n",
    "    ax.text(9, -0.94, 'Overfitting', fontsize=22)\n",
    "    ax.text(3, -0.94, 'Underfitting', fontsize=22)\n",
    "    ax.axvline(7, ls='--')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbisFujO-fwB"
   },
   "source": [
    "We run the plotting for between 2 and 13 parameters, note that the optimal is between 7 and 8 parameters, I already included this line in the above plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pov4Rt6H9hx4"
   },
   "outputs": [],
   "source": [
    "plot_validation_curve(clf_1, X_train, y_train, param_name='max_depth', param_range=range(2,13))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TGmgY6pY-xEa"
   },
   "source": [
    "2nd example over/underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCIceTkI_Ep3"
   },
   "source": [
    "We evaluate quantitatively overfitting / underfitting by using cross-validation. We calculate the mean squared error (MSE) on the validation set, the higher, the less likely the model generalizes correctly from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JQDnOc769kt4"
   },
   "outputs": [],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "for i in range(len(degrees)):\n",
    "    ax = plt.subplot(1, len(degrees), i + 1)\n",
    "    plt.setp(ax, xticks=(), yticks=())\n",
    "\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label=\"Model\")\n",
    "    plt.plot(X_test, true_fun(X_test), label=\"True function\")\n",
    "    plt.scatter(X, y, edgecolor='b', s=20, label=\"Samples\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.xlim((0, 1))\n",
    "    plt.ylim((-2, 2))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Degree {}\\nMSE = {:.2e}(+/- {:.2e})\".format(\n",
    "        degrees[i], -scores.mean(), scores.std()))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Ensemble Methods.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
